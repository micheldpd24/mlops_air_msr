# Project Overview
Design and Implementation of an MLOps CI/CD Pipeline for a Music Recommendation System
- Developed a continuous integration and continuous deployment (CI/CD) pipeline for a music recommendation system leveraging Spotify music playlists. The pipeline's design is based on the methodology from the Datascientest MLOps wine quality competition project [add link].
- Pipeline Orchestration with Apache Airflow
Configured and managed the orchestration of the pipeline using Apache Airflow, enabling automated, scalable, and efficient workflows.

- Experiment Monitoring with MLflow and Dagshub
Integrated MLflow for tracking machine learning experiments, model parameters, and performance metrics. With MLflow server setup in DagsHub.

- Containerization with Docker
Dockerized both the recommendation application and the pipeline orchestration, ensuring portability, scalability, and consistency across environments.

# Setup - what you need to do to reproduce this project:

1st clone the repos

## Python virtual environment
- python3 -m venv .venv # to create a virtual .venv python environnment for the project
- source .venv/bin/activate # to activate the virtual environment (in linux or mac os environments)
- python pip install -r requirements.txt  # to install all the packages needed for the project

## Docker
- Make sure you have Docker installed on you system

## Spotify  / DagsHub / Mlflow id
- get Spotify Client ID and Spotify CLient Secret from [Spotify for Developpers](https://developer.spotify.com/).
- Create a Dagshub Repo for models experiments tracking with MLflow, make sure to get the MLFLOW URI
- Open the file mlops_msr/mlflow_and_sp.env and input you id, secret and mlflow uri:
  
  MLFLOW_TRACKING_URI=https://dagshub.com/<your username>/<your repos name>.mlflow
  MLFLOW_TRACKING_USERNAME=<your dagshub username>
  MLFLOW_TRACKING_PASSWORD=<your mlflow tracking password>
  CLIENT_ID=<your spotify api client id>
  CLIENT_SECRET=<your spotify api client secret>
After completion,  make sure to save the changes.
- Unzip playlist.json.zip into mlops_msr/data/raw/playlist.json and delete the zip file
- make sure you have dataset.zip in mlops_air_msr/mlops_msr/data/raw/. Since it is the original dataset of spotify songs to be used to initialize the pipeline

## Running the pipeline
First run this command from the root of your porject folder:


# Setting up an MLOps project step by step üöÄ

Welcome to the setup guide! Here, we'll outline the steps needed to configure and implement the various first stages of the MLOps pipeline. Follow along and fill in the details as you proceed through each step in the `workflow_steps.ipynb` notebook.

You can start by getting familiar with the architecture of the project: 

```bash
.
‚îú‚îÄ‚îÄ config/
‚îú‚îÄ‚îÄ dags/
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ custom_logger.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dag_data_feeding.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dag_utils.py
‚îÇ   ‚îÇ¬†¬† 
‚îú‚îÄ‚îÄ logs/
‚îÇ¬†¬† ‚îÇ¬†¬†    ¬† 
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ logs.log
‚îÇ
‚îú‚îÄ‚îÄ mlops_msr
‚îÇ¬†¬† ‚îÇ  
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ interim/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† 
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models_best/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† 
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ new/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† 
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ processed/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ X_test.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ X_train.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ playlist_df.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ y_test.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ y_train.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ raw/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data_feeding.log
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dataset.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dataset.zip
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ playlists.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ retrain.log
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ retrain_logs.log
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ song_df.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ reco/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ recommendation_data.json
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ genres.txt
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec_0.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec_1.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec_2.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec_3.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec_4.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec_5.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec_6.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec_7.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ to_rec_8.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ to_rec_9.csv
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ uris/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ status.txt
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ images/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ cosine_s.jpg
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ logs/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ logs.log
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ metrics.
‚îÇ¬†¬† ‚îÇ¬†¬† 
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ mlruns/
‚îÇ¬†¬† ‚îÇ¬†¬† 
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models/
‚îÇ¬†¬† ‚îÇ¬†¬† 
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models_best/
‚îÇ¬†¬† ‚îÇ¬†¬† 
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ notebooks/
‚îÇ¬†¬† ‚îÇ¬†
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ shell_scripts_dk/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ run_data_feeding.sh
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ run_pipe.sh
‚îÇ¬†¬† ‚îÇ¬†
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ src/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ app/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†  ‚îÇ¬†¬† 
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ logs
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ static
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ templates
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ app.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ app_utils.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ reco_monitoring.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†  
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data_module_def/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†  ‚îú‚îÄ‚îÄ __pycache__
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data_ingestion.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data_transformation.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data_validation.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ schema.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†  
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ models_module_def/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __pycache__
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ model_evaluation.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ model_trainer.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ params.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ unsmodel_fit.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†  
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ pipeline_steps/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __pycache__
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ prediction.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ prediction_old_nogit.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ stage01_data_ingestion.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ stage02_data_validation.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ stage03_data_transformation.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ stage04_model_trainer.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ stage05_model_evaluation.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ stage06_uns_model_fit_eval.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îÇ
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ common_utils.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config.yaml
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ config_manager.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ data_feeding.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îú‚îÄ‚îÄ entity.py
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ launch_retrain.py
‚îÇ¬†¬† ‚îÇ
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ users/
‚îÇ¬†¬† ‚îÇ¬†¬† ‚îî‚îÄ‚îÄ users.json
‚îÇ¬†¬† ‚îÇ¬†
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ Dockerfile
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ __init__.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ app_requirements.txt
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ custom_logger.py
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ docker-compose.yaml
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dvc.lock
‚îÇ¬†¬† ‚îú‚îÄ‚îÄ dvc.yaml
‚îÇ¬†¬† ‚îî‚îÄ‚îÄ main.py
‚îÇ¬†¬† 
‚îú‚îÄ‚îÄ plugins/
‚îÇ¬†¬† 
‚îú‚îÄ‚îÄ tests/
‚îÇ
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ air_requirements.txt
‚îú‚îÄ‚îÄ docker-compose.yaml
‚îî‚îÄ‚îÄ requirements.txt
``

Through this project we'll work with a songs dataset. The goal is implement a recommendation system that will recommend a number of songs given a spotify music playlist. All while adhering to the best practices in MLOps in terms of version control, use of pipelines and the most commonly used tools.

- The recommender application is a Flask API
- Models training and Evaluation are monitored with MLflow with the MLflow Server in DagsHub
- The Whole Pipeline Orchestration from Data feeding from Spotify to models training nd Evaluation is done using Apache Airflow

## Virtual Environement

First of all you need to start by forking and cloning the project. Then, you must create a virtual environment where you'll install all the necessary libraries. These can be found in the `requirements.txt` file üìö Make sure you activate your virtual environment before you use it üòâ

## Launch Airflow Docker and Music Recommander Flask API
From the roots of the project folder execute the command:  docker compose up --build

-- The recommmender API "rec-app-air" is accessible from http:://localhost:50000
-- Airflow UI is accessible from https://localhost:8080

Now let's go through the files that are readily available.

## Pipeline Description
[TBD]


## Configuration Files üìò
Let's have a quick look at the three `yaml` files in our `src` folder.

You can start by having a look at the `config.yaml` üìÇ You will see that it sets the paths to the different files that will be used and created in each of the steps we'll put in place.

Next, inside the `data_module_def` folder we have the `schema.yaml` üóÉÔ∏è If you have a look at it you'll see it defines the data types for each column in the dataset we'll work with.

Finally, inside the `models_module_def` folder  you can have a look at `params.yaml` üìä What this file does is set the hyperparameters of the model we'll put in place.

‚ö†Ô∏è The file `src/config.py` defines the global variables containing the paths to these yaml files to facilitate their access. 

## Common Utilities üõ†Ô∏è 
In `src/common_utils.py` we have reusable functions:

* read_yaml(filepath: str) -> dict
* create_directories(paths: List[str])
* save_json(path: str, data: dict)
* load_json

These utilities will streamline the loading of configurations and ensure necessary directories are created.

Let' get to work!

## The task
For the next steps you can use the notebook `workflow_steps.ipynb` to guide you through the code you'll need to write on each of the corresponding files üßë‚Äçüíª The task consist of five steps which will help you implement a modularized workflow of an MLOps project.

## Step 1: Define Configuration Classes üß©
Start by writing the configuration objects in `src/entity.py`. These configurations will help in managing the settings and parameters required for each stage in a clean and organized manner. Using the *Step 1* section in the notebook, define `dataclasses` for configuration objects:

* DataIngestionConfig
* DataValidationConfig
* DataTransformationConfig
* ModelTrainerConfig
* ModelEvaluationConfig
* UnsModelFitConfig

## Step 2: Configuration Manager üóÑÔ∏è
Create the class `ConfigurationManager` in `src/config_manager.py` using the *Step 2* of the notebook. This class will:

* Read paths from `config.yaml`.
* Read hyperparameters from `params.yaml`.
* Read the data types from `schema.yaml`.
* Create configuration objects for each of the stages through the help of the objects defined on the step before: DataIngestionConfig, DataValidationConfig, ModelTrainerConfig and ModelEvaluationConfig.
* Create necessary folders.

‚ö†Ô∏è Pay attention to the `mlflow_uri` on the `get_model_evaluation_config`, make sure you adapt it with your own dagshub credentials. 

## Step 3: Data module definition and model module definition.
Using the *Step 3* section of the notebook, in the corresponding files of the  `src/data_module_def` folder, create:

1. Data Ingestion module üì•

This class will:
* Download the dataset into the appropriate folder.
* Unzip the dataset into the appropriate folder.

2. Data Validation module ‚úÖ

This class will:
* Validate columns against the schema. Optional: you can also verify the informatic type.
* Issue a text file saying if the data is valid.

3. Data Transformation module üîÑ

This class will:
* Split the data into training and test sets.
* Save the corresponding csv files into the appropriate folder.

Similarly, in the corresponding files of the `src/models_module_def` folder, create:

1. Model trainer module üèãÔ∏è‚Äç‚ôÇÔ∏è

This class will:
* Train the model using the hyperparameters specified in `params.yaml`.
* Save the trained model into the appropriate folder.

2. Model Evaluation module üìù

This class will
* Evaluate the model and log metrics using MLFlow

3. Unsupervised Model Fit and Evaluation module üìù
This class will
* Fit a clustering model for each music genre 
* Evaluate each fitted model and log metrics using MLFlow 

## Step 4: Pipeline Steps üöÄ
Using the *Step 4* of the notebook, in `src/pipeline_steps` create scripts for each stage of the pipeline to instantiate and run the processes:

* stage01_data_ingestion.py
* stage02_data_validation.py
* stage03_data_transformation.py
* stage04_model_trainer.py
* stage05_model_evaluation.py
* stage06_uns_model_fit_eval.py

On each script you have to complete the classes with two methods: an `__init__` that doesn't do anything, and a `main` where you have to implement the code in each section of the *Step 4* of the notebook.

## Step 5: Use DVC to connect the different stages of your pipeline ü¶â
Start by setting DagsHub as your distant storage through DVC.

```bash
dvc remote add origin s3://dvc
dvc remote modify origin endpointurl https://dagshub.com/your_username/your_repo.s3 
dvc remote default origin
```

Use dvc to connect the different steps of your pipeline.

For example, the command for addind the first step of the pipeline is: 

```bash
dvc run -n data_ingestion \
  -d src/config.yaml \
  -d src/pipeline_steps/stage01_data_ingestion.py \
  -o data/raw/song_df.csv \
  -c 'python src/pipeline_steps/stage01_data_ingestion.py'

```
Add the following steps for the data transformation, data validation, model training and model evaluation.

You can run the pipeline through the command `dvc repro`.

Congratulations! üéâ Now that you have a structured and well-defined MLOps project you're ready for the next step which is the creation of the API.

Each step is modularized, making it easy to maintain, extend, and scale your Machine Learning pipeline. 

# Pipeline Orchestration with Apache Airflow

